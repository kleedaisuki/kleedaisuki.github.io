---
title: "AI Model Selection: Domestic Solutions and Open-Source Alternatives"
description: "There is no 'perfect,' only 'most suitable'; to dance within the boundaries of possibility under real-world constraints is precisely the charm of computer science~"
date: "2025-08-26"
locale: "en"
slug: "2025-08-26-1-en"  
tags: ["Model Selection", "AI Engineering", "Open-Source Models"]
draft: false
pair: "2025-08-26-1-zh"
---
Although top-tier flagship models like ChatGPT, Claude, or Gemini are practically unavailable, and we face severe compliance challenges in corporate or team settings, domestic solutions and open-source alternatives are not without potential. While they can't fully reach the heights of these flagships, they are "good enough" and cost-effective.

DeepSeek has released its latest v3.1, continuing its low-profile style from May when it updated the R1 weights without changing the version number. Although this 3.1 version is sometimes considered an "overachiever" that excels on benchmarks but falters in practice, its low price, the demystification of reasoning, and the "speed always wins" philosophy all reflect the team's taste for cutting-edge technology. It's more a matter of finding the right use case—its API calls are excellent for handling daily needs and general tasks.

For coding productivity, Kimi K2's outstanding performance in agentic abilities would actually be a better choice. A mature setup combining CLI tools like Claude Code with Kimi K2 offers a suitable trade-off between price and performance. Moreover, the hassles of a Claude Pro subscription, along with its strict network and payment requirements, create a significant barrier that discourages many users. Just like Europe's overly strict safety policies, "Constitutional AI" can sometimes be quite constrained.

Another giant in domestic open-source models is Qwen, which, alongside Gemini, has been praised as a managerial miracle—Meta's LLaMA really set a poor precedent. Qwen's embedding models have always been an excellent choice; they are highly recommended for building vector databases for Retrieval-Augmented Generation (RAG) or other broader tasks.

Also noteworthy for performance-intensive tasks is OpenAI's own open-source model, gpt-oss, which reaches the level of o4-mini and even o3-mini. (This is quite high; even the weight-updated DeepSeek-R1-0528 is still some distance from o4-mini, a fellow reasoning model, representing a situation where the 'floor' is higher than the 'ceiling'.) Therefore, for performance-demanding needs, purchasing cloud services to deploy this open-source model is a great option.

Now, for some very niche but very real needs, we have Gemma 3n. This mobile-first open-source model is incredible: its ultra-compact sub-1B parameter version still supports multimodality! This is an absolute blessing for mobile computing and real-time translation.

In conclusion, as we arrive in August 2025, although the debate between open-source and closed-source models was decisively settled last year in a rather one-sided fashion, this doesn't stop us from using flexible engineering skills and academic methods to build our own systems. For instance, the paper *[The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants](https://arxiv.org/html/2505.19797v1)* uses clustering algorithms with open-source models to achieve results surpassing GPT-4.1 (effectively creating a sort of explicit Mixture of Experts (MoE) architecture). This consistently highlights the essence of engineering: there is no "perfect," only "most suitable." To dance within the boundaries of possibility under real-world constraints is precisely the charm of Computer Science.
