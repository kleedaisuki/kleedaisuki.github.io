---
title: "Awesome Article Sharing!"
description: "This represents a paradigm shift in scientific research. With PUCT (Predictor + UCB applied to Trees), the core of tackling these tasks completely shifts from solving problems to finding them, transforming into a quest to design perfect metrics that accurately reflect scientific goals."
date: "2025-09-13"
locale: "en"
slug: "2025-09-13-1-en"
tags: ["Paper Reading", "AI Systems", "Musings"]
draft: false
pair: "2025-09-13-1-zh"
---

A very interesting paper, *[An AI system to help scientists write expert-level empirical software](https://arxiv.org/abs/2509.06503v1)*. Here are some of my thoughts:

1. My Gemini put it perfectly: The impact of a study largely depends on how you define and frame the problem it aims to solve. Elevating a specific issue into a universal challenge is a key step to multiply the impact of your research. The authors didn't just say they "found a better code generation method for several benchmark tasks"; instead, they claimed they are "accelerating the loop of scientific discovery." This difference in perspective is what separates a good paper from a top-tier one. It shouldn't be just about "solving a problem." You need to learn how to frame your problem, how to generalize your solution, and how to design a convincing evaluation strategy to prove your point. The narrative structure and chain of evidence in a paper are just as important as the technical innovation itself.

2. Again, my Gemini nailed it: You don't necessarily have to invent a brand-new theory or algorithm from scratch. A profound contribution can absolutely come from discovering a new way to combine existing, powerful tools to solve a problem that is far beyond the reach of any single tool. You should always maintain a cross-disciplinary perspective and think, "What would happen if I applied this powerful technique from field A to that classic problem in field B?" This ability to discover new connections and create new combinations is a vital source of innovation.

3. This is a paradigm shift in scientific research. With this PUCT (Predictor + UCB applied to Trees) approach for these kinds of tasks, the core focus radically shifts from problem-solving to problem-finding. It becomes about designing the perfect metrics that accurately reflect scientific goals. In the past, one might have achieved this kind of innovation by curating a dataset or controlling the definition of standards (optimization targets) and then grinding to achieve State-of-the-Art (SOTA) results. However, under this new path, the lifecycle of such an approach has been drastically shortened. The ability to define the direction and new paths for optimization is crucial in any era. Now, with some creative destruction, the old way of being an "executor" held captive by performance metrics—that is, the 1-to-100 innovation within an existing paradigm—is no longer viable, because you can't possibly compete in execution against an indefatigable AI system.

4. In reality, only large institutions can afford to play with this kind of AI system. For an individual, tackling such a hyper-complex system is highly impractical in terms of both engineering effort and resources. Whoever can first establish the computational power, data, and implementation pipeline for this AI system will dominate the next era of scientific research. This is the throne for setting the new paradigm. Ultimately, we must adapt to the transition between old and new paradigms and to creative destruction. The old way of doing research—merely chasing publication numbers and obeying the authority of a supervisor—is truly becoming worthless. Therefore, for an individual, being "AI-native" is far more practical than acquiring more traditional academic skills. Although everyone pretends not to see the elephant in the room and continues with the old methods of training, merely treating AI as a new direction for innovation and publication, the cycles and laws of change will not be easily swayed by human will.

5. We should demystify quantifiable metrics. In the past, an over-reliance on them was a trade-off for efficiency. But now, on a tooling level, we have much better options. No matter what, we must adapt to the changing standards of evaluation—hiring more and more "executors" will never allow you to compete with an explorer who can pose new questions and navigate ambiguous territories. The obsession with creating all sorts of standardized tests for selection may seem reasonable, but from the perspective of the old guard, it just leads to Goodhart's Law playing out over and over again. So, don't complain that the people and students you recruit are only good at taking tests but can't get things done or translate research into practice. First, reflect on just how poor your own taste and judgment have become.
